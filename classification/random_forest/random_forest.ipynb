{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class DecisionNode:\n",
    "  def __init__(self, impurity=None, feature_index=None, threshold=None, left=None, right=None):\n",
    "    self.left = left\n",
    "    self.right = right\n",
    "    # The largest impurity value of this node\n",
    "    self.impurity = impurity\n",
    "    # Index of the feature which make the best fit for this node.\n",
    "    self.feature_index = feature_index\n",
    "    # The threshold value for that feature to make the split.\n",
    "    self.threshold = threshold\n",
    "\n",
    "class LeafNode:\n",
    "  def __init__(self, value):\n",
    "    self.prediction_value = value\n",
    "\n",
    "class DecisionTreeClassifierFromScratch:\n",
    "  def __init__(self, min_sample_split=3, min_impurity=1e-7, max_depth=10, criterion='gini'):\n",
    "    self.root = None\n",
    "    self.min_sample_split = min_sample_split\n",
    "    self.min_impurity = min_impurity\n",
    "    self.max_depth = max_depth\n",
    "    self.impurity_function = self._claculate_information_gain\n",
    "    if criterion == 'entropy':\n",
    "      self.criterion = self._entropy\n",
    "      self.criterion_name = criterion\n",
    "    else:\n",
    "      self.criterion = self._gini_index\n",
    "      self.criterion_name = 'gini'\n",
    "\n",
    "  def _gini_index(self, y):\n",
    "    gini = 1\n",
    "    unique_value = np.unique(y)\n",
    "    for val in unique_value:\n",
    "      # probability of that class.\n",
    "      p = np.sum(y == val) / len(y)\n",
    "      gini += -np.square(p)\n",
    "    return gini\n",
    "\n",
    "  def _entropy(self, y):\n",
    "    entropy = 0\n",
    "    unique_value = np.unique(y)\n",
    "    for val in unique_value:\n",
    "      # probability of that class.\n",
    "      p = np.sum(y == val) / len(y)\n",
    "      entropy += -p * np.log2(p)\n",
    "    return entropy\n",
    "\n",
    "  def _claculate_information_gain(self, y, y1, y2):\n",
    "    # :param y: target value.\n",
    "    # :param y1: target value for dataset in the true split/right branch.\n",
    "    # :param y2: target value for dataset in the false split/left branch.\n",
    "\n",
    "    # propobility of true values.\n",
    "    p = len(y1) / len(y)\n",
    "    info_gain = self.criterion(y) - p * self.criterion(y1) - (1 - p) * self.criterion(y2)\n",
    "    return info_gain\n",
    "\n",
    "  def _leaf_value_calculation(self, y):\n",
    "    most_frequent_label = None\n",
    "    max_count = 0\n",
    "    unique_labels = np.unique(y)\n",
    "    # iterate over all the unique values and find their frequentcy count.\n",
    "    for label in unique_labels:\n",
    "      count = len( y[y == label])\n",
    "      if count > max_count:\n",
    "        most_frequent_label = label\n",
    "        max_count = count\n",
    "    return most_frequent_label\n",
    "\n",
    "  def _partition_dataset(self, Xy, feature_index, threshold):\n",
    "    col = Xy[:, feature_index]\n",
    "    X_1 = Xy[col >= threshold]\n",
    "    X_2 = Xy[col < threshold]\n",
    "\n",
    "    return X_1, X_2\n",
    "\n",
    "  def _find_best_split(self, Xy):\n",
    "    best_question = tuple()\n",
    "    best_datasplit = {}\n",
    "    largest_impurity = 0\n",
    "    n_features = (Xy.shape[1] - 1)\n",
    "    # iterate over all the features.\n",
    "    for feature_index in range(n_features):\n",
    "      # find the unique values in that feature.\n",
    "      unique_value = set(s for s in Xy[:,feature_index])\n",
    "      # iterate over all the unique values to find the impurity.\n",
    "      for threshold in unique_value:\n",
    "        # split the dataset based on the feature value.\n",
    "        true_xy, false_xy = self._partition_dataset(Xy, feature_index, threshold)\n",
    "\n",
    "        # skip the node which has any on type 0. because this means it is already pure.\n",
    "        if len(true_xy) > 0 and len(false_xy) > 0:\n",
    "          # find the y values.\n",
    "          y = Xy[:, -1]\n",
    "          true_y = true_xy[:, -1]\n",
    "          false_y = false_xy[:, -1]\n",
    "          # calculate the impurity function.\n",
    "          impurity = self.impurity_function(y, true_y, false_y)\n",
    "\n",
    "          # if the calculated impurity is larger than save this value for comaparition (higest gain).\n",
    "          if impurity > largest_impurity:\n",
    "            largest_impurity = impurity\n",
    "            best_question = (feature_index, threshold)\n",
    "            best_datasplit = {\n",
    "              \"leftX\": true_xy[:, :n_features],   # X of left subtree\n",
    "              \"lefty\": true_xy[:, n_features:],   # y of left subtree\n",
    "              \"rightX\": false_xy[:, :n_features],  # X of right subtree\n",
    "              \"righty\": false_xy[:, n_features:]   # y of right subtree\n",
    "            }\n",
    "\n",
    "    return largest_impurity, best_question, best_datasplit\n",
    "\n",
    "  def _build_tree(self, X, y, current_depth=0):\n",
    "    n_samples , n_features = X.shape\n",
    "    # Add y as last column of X\n",
    "    Xy = np.column_stack((X, y))\n",
    "    # find the Information gain on each feature each values and return the question which splits the data very well\n",
    "    if (n_samples >= self.min_sample_split) and (current_depth < self.max_depth):\n",
    "      # find the best split/ which question split the data well.\n",
    "      impurity, quesion, best_datasplit = self._find_best_split(Xy)\n",
    "      if impurity > self.min_impurity:\n",
    "        # Build subtrees for the right and left branch.\n",
    "        true_branch = self._build_tree(best_datasplit[\"leftX\"], best_datasplit[\"lefty\"], current_depth + 1)\n",
    "        false_branch = self._build_tree(best_datasplit[\"rightX\"], best_datasplit[\"righty\"], current_depth + 1)\n",
    "        return DecisionNode(impurity=impurity, feature_index=quesion[0], threshold=quesion[1],\n",
    "                            left=true_branch, right=false_branch)\n",
    "\n",
    "    leaf_value = self._leaf_value_calculation(y)\n",
    "    return LeafNode(value=leaf_value)\n",
    "\n",
    "  def fit(self, X, y):\n",
    "    self.root = self._build_tree(X, y, current_depth=0)\n",
    "\n",
    "  def predict_sample(self, x, tree=None):\n",
    "    if isinstance(tree , LeafNode):\n",
    "      return tree.prediction_value\n",
    "\n",
    "    if tree is None:\n",
    "      tree = self.root\n",
    "    feature_value = x[tree.feature_index]\n",
    "    branch = tree.right\n",
    "\n",
    "    if isinstance(feature_value, int) or isinstance(feature_value, float):\n",
    "      if feature_value >= tree.threshold:\n",
    "        branch = tree.left\n",
    "    elif feature_value == tree.threshold:\n",
    "      branch = tree.left\n",
    "\n",
    "    return self.predict_sample(x, branch)\n",
    "\n",
    "  def predict(self, test_X):\n",
    "    x = np.array(test_X)\n",
    "    y_pred = [self.predict_sample(sample) for sample in x]\n",
    "    y_pred = np.array(y_pred)\n",
    "    return y_pred\n",
    "\n",
    "  def draw_tree(self):\n",
    "    self._draw_tree(self.root)\n",
    "\n",
    "  def _draw_tree(self, tree = None, indentation = \" \", depth=0):\n",
    "    if isinstance(tree , LeafNode):\n",
    "      print(indentation,\"The predicted value -->\", tree.prediction_value)\n",
    "      return\n",
    "    else:\n",
    "      print(indentation,f\"({depth}) Is {tree.feature_index}>={tree.threshold}?\"\n",
    "            f\": {self.criterion_name}:{tree.impurity:.2f}\")\n",
    "      if tree.left is not None:\n",
    "          print (indentation + '----- True branch :)')\n",
    "          self._draw_tree(tree.left, indentation + \"  \", depth+1)\n",
    "      if tree.right is not None:\n",
    "          print (indentation + '----- False branch :)')\n",
    "          self._draw_tree(tree.right, indentation + \"  \", depth+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestClassifierFromScratch:\n",
    "  def __init__(self, max_feature=None, n_trees=100, min_sample_split=2, min_impurity=1e-7, max_depth=10, criterion='gini'):\n",
    "    # Initialize the trees.\n",
    "    self.trees = []\n",
    "    for _ in range(n_trees):\n",
    "      self.trees.append(DecisionTreeClassifierFromScratch(min_sample_split=min_sample_split,min_impurity=min_impurity,\n",
    "                                                          max_depth=max_depth,criterion=criterion))\n",
    "\n",
    "    self.tree_feature_indexes = []\n",
    "    # Number of trees/estimetors.\n",
    "    self.n_estimators = n_trees\n",
    "    # How many features can be used for a tree from the whole features.\n",
    "    self.max_features = max_feature\n",
    "    # Aggication function to find the prediction.\n",
    "    self.prediction_aggrigation_calculation = self._maximum_vote_calculation\n",
    "\n",
    "  def _maximum_vote_calculation(self, y_preds):\n",
    "    # Find which prediction class has higest frequency in all tree prediction for each sample.\n",
    "    # create a empty array to store the prediction.\n",
    "    y_pred = np.empty((y_preds.shape[0], 1))\n",
    "    # iterate over all the data samples.\n",
    "    for i, sample_predictions in enumerate(y_preds):\n",
    "      y_pred[i] = np.bincount(sample_predictions.astype('int')).argmax()\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "  def _make_random_subset(self, X, y, n_subsets, replacement=True):\n",
    "    # Create a random subset of dataset with/without replacement.\n",
    "    subset = []\n",
    "    # use 100% of data when replacement is true , use 50% otherwise.\n",
    "    sample_size = (X.shape[0] if replacement else (X.shape[0] // 2))\n",
    "\n",
    "    # Add y as last column of X\n",
    "    Xy = np.column_stack((X, y))\n",
    "    np.random.shuffle(Xy)\n",
    "    # Select randome subset of data with replacement.\n",
    "    for i in range(n_subsets):\n",
    "      index = np.random.choice(range(sample_size), size=np.shape(range(sample_size)), replace=replacement)\n",
    "      X = Xy[index][:, :-1]\n",
    "      y = Xy[index][: , -1]\n",
    "      subset.append({\"X\" : X, \"y\": y})\n",
    "    return subset\n",
    "\n",
    "  def fit(self, X, y):\n",
    "    # if the max_features is not given then select it as square root of no on feature availabe.\n",
    "    n_features = X.shape[1]\n",
    "    if self.max_features == None:\n",
    "      self.max_features = int(round(np.sqrt(n_features)))\n",
    "\n",
    "    # Split the dataset into number of subsets equal to n_estimators.\n",
    "    subsets = self._make_random_subset(X, y, self.n_estimators)\n",
    "\n",
    "    for i, subset in enumerate(subsets):\n",
    "      X_subset , y_subset = subset[\"X\"], subset[\"y\"]\n",
    "      # select a random sucset of features for each tree. This is called feature bagging.\n",
    "      idx = np.random.choice(range(n_features), size=self.max_features, replace=True)\n",
    "      # track this for prediction.\n",
    "      self.tree_feature_indexes.append(idx)\n",
    "      # Get the X with the selected features only.\n",
    "      X_subset = X_subset[:, idx]\n",
    "\n",
    "      # change the y_subet to i dimentional array.\n",
    "      y_subset = np.expand_dims(y_subset, axis =1)\n",
    "      # build the model with selected features and selected random subset from dataset.\n",
    "      self.trees[i].fit(X_subset, y_subset)\n",
    "\n",
    "  def predict(self, test_X):\n",
    "    y_preds = np.empty((test_X.shape[0], self.n_estimators))\n",
    "    # find the prediction from each tree for each samples\n",
    "    for i, tree in enumerate(self.trees):\n",
    "      features_index = self.tree_feature_indexes[i]\n",
    "      X_selected_features = test_X[:, features_index]\n",
    "      if isinstance(tree, DecisionTreeClassifier):\n",
    "        y_preds[:, i] = tree.predict(X_selected_features).reshape((-1,))\n",
    "      else:\n",
    "        y_preds[:, i] = tree.predict(X_selected_features)\n",
    "    # find the aggregated output.\n",
    "    y_pred = self.prediction_aggrigation_calculation(y_preds)\n",
    "\n",
    "    return y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
