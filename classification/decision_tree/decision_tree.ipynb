{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZlSeix_wg30m"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Matplotlib is building the font cache; this may take a moment.\n"
          ]
        }
      ],
      "source": [
        "# Homework 4_1\n",
        "# ข้อมูล\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [10,6]\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score,recall_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import joblib\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('dynamic_supply_chain_logistics_dataset.csv')\n",
        "df.drop(columns=['timestamp', 'vehicle_gps_latitude', 'vehicle_gps_longitude'], inplace=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAC4O-MlhB43"
      },
      "outputs": [],
      "source": [
        "# Look if there are any duplicate rows in the dataset\n",
        "df.loc[df.duplicated()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for null values\n",
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "numerical_list = [x for x in df.columns if df[x].dtype in ('int64','float64')]\n",
        "print(numerical_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Removal of outlier:\n",
        "df1 = df.copy()\n",
        "\n",
        "for i in numerical_list:\n",
        "    Q1 = df1[i].quantile(0.25)\n",
        "    Q3 = df1[i].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    df1 = df1[df1[i] <= (Q3+(1.5*IQR))]\n",
        "    df1 = df1[df1[i] >= (Q1-(1.5*IQR))]\n",
        "    df1 = df1.reset_index(drop=True)\n",
        "# display(df1.head())\n",
        "print('\\n\\033[1mInference:\\033[0m\\nBefore removal of outliers, The dataset had {} samples.'.format(df.shape[0]))\n",
        "print('After removal of outliers, The dataset now has {} samples.'.format(df1.shape[0]))\n",
        "df = df1.copy()\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Label encoding\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "df['risk_classification'] = label_encoder.fit_transform(df['risk_classification'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# target class are highly imbalanced\n",
        "df['risk_classification'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# solve imbalnce class\n",
        "\n",
        "#Random Oversampling\n",
        "# from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# ros = RandomOverSampler(sampling_strategy=\"not majority\") # String\n",
        "# X = df.drop(['risk_classification'], axis=1)\n",
        "# Y = df['risk_classification']\n",
        "# X, Y = ros.fit_resample(X, Y)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# Apply SMOTE (Synthetic Minority Over-sampling)\n",
        "from imblearn.over_sampling import SMOTE\n",
        "X = df.drop(columns=[\"risk_classification\"])  # Features\n",
        "Y = df[\"risk_classification\"]  # Target\n",
        "smote = SMOTE(sampling_strategy=\"auto\", random_state=42)\n",
        "X, Y = smote.fit_resample(X, Y)\n",
        "\n",
        "ax = Y.value_counts().plot.pie(autopct='%.2f')\n",
        "_ = ax.set_title(\"Over-sampling\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# after solve imbalnce class\n",
        "Y.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size = 0.25,random_state = 42)\n",
        "\n",
        "#  Select Features Using Information Gain For Classification In ML\n",
        "\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "# determine the mutual information\n",
        "mutual_info = mutual_info_classif(X_train, y_train)\n",
        "mutual_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mutual_info = pd.Series(mutual_info)\n",
        "mutual_info.index = X_train.columns\n",
        "mutual_info.sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#let's plot the ordered mutual_info values per feature\n",
        "mutual_info.sort_values(ascending=False).plot.bar(figsize=(20, 8))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest\n",
        "\n",
        "#No we Will select the  top 5 important features\n",
        "sel_five_cols = SelectKBest(mutual_info_classif, k=5)\n",
        "sel_five_cols.fit(X_train, y_train)\n",
        "selected_feature = X_train.columns[sel_five_cols.get_support()]\n",
        "print(f\"Top 5 Feature: {selected_feature}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train = X_train[selected_feature]\n",
        "X_test = X_test[selected_feature]\n",
        "\n",
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transform data\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "print(X_train_scaled)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pf3c8gihEtA"
      },
      "outputs": [],
      "source": [
        "class DecisionNode:\n",
        "  def __init__(self, impurity=None, feature_index=None, threshold=None, left=None, right=None):\n",
        "    self.left = left\n",
        "    self.right = right\n",
        "    # The largest impurity value of this node\n",
        "    self.impurity = impurity\n",
        "    # Index of the feature which make the best fit for this node.\n",
        "    self.feature_index = feature_index\n",
        "    # The threshold value for that feature to make the split.\n",
        "    self.threshold = threshold\n",
        "\n",
        "class LeafNode:\n",
        "  def __init__(self, value):\n",
        "    self.prediction_value = value\n",
        "\n",
        "class DecisionTreeClassifierFromScratch:\n",
        "  def __init__(self, min_sample_split=3, min_impurity=1e-7, max_depth=10, criterion='gini'):\n",
        "    self.root = None\n",
        "    self.min_sample_split = min_sample_split\n",
        "    self.min_impurity = min_impurity\n",
        "    self.max_depth = max_depth\n",
        "    self.impurity_function = self._calculate_information_gain\n",
        "    if criterion == 'entropy':\n",
        "      self.criterion = self._entropy\n",
        "      self.criterion_name = criterion\n",
        "    else:\n",
        "      self.criterion = self._gini_index\n",
        "      self.criterion_name = 'gini'\n",
        "\n",
        "  def _gini_index(self, y):\n",
        "    gini = 1\n",
        "    unique_value = np.unique(y)\n",
        "    for val in unique_value:\n",
        "      # probability of that class.\n",
        "      p = np.sum(y == val) / len(y)\n",
        "      gini += -np.square(p)\n",
        "    return gini\n",
        "\n",
        "  def _entropy(self, y):\n",
        "    entropy = 0\n",
        "    unique_value = np.unique(y)\n",
        "    for val in unique_value:\n",
        "      # probability of that class.\n",
        "      p = np.sum(y == val) / len(y)\n",
        "      entropy += -p * np.log2(p)\n",
        "    return entropy\n",
        "\n",
        "  def _calculate_information_gain(self, y, y1, y2):\n",
        "    # :param y: target value.\n",
        "    # :param y1: target value for dataset in the true split/right branch.\n",
        "    # :param y2: target value for dataset in the false split/left branch.\n",
        "\n",
        "    # propobility of true values.\n",
        "    p = len(y1) / len(y)\n",
        "    info_gain = self.criterion(y) - p * self.criterion(y1) - (1 - p) * self.criterion(y2)\n",
        "    return info_gain\n",
        "\n",
        "  def _leaf_value_calculation(self, y):\n",
        "    most_frequent_label = None\n",
        "    max_count = 0\n",
        "    unique_labels = np.unique(y)\n",
        "    # iterate over all the unique values and find their frequentcy count.\n",
        "    for label in unique_labels:\n",
        "      count = len( y[y == label])\n",
        "      if count > max_count:\n",
        "        most_frequent_label = label\n",
        "        max_count = count\n",
        "    return most_frequent_label\n",
        "\n",
        "  def _partition_dataset(self, Xy, feature_index, threshold):\n",
        "    col = Xy[:, feature_index]\n",
        "    X_1 = Xy[col >= threshold]\n",
        "    X_2 = Xy[col < threshold]\n",
        "\n",
        "    return X_1, X_2\n",
        "\n",
        "  def _find_best_split(self, Xy):\n",
        "    best_question = tuple()\n",
        "    best_datasplit = {}\n",
        "    largest_impurity = 0\n",
        "    n_features = (Xy.shape[1] - 1)\n",
        "    # iterate over all the features.\n",
        "    for feature_index in range(n_features):\n",
        "      # find the unique values in that feature.\n",
        "      unique_value = set(s for s in Xy[:,feature_index])\n",
        "      # iterate over all the unique values to find the impurity.\n",
        "      for threshold in unique_value:\n",
        "        # split the dataset based on the feature value.\n",
        "        true_xy, false_xy = self._partition_dataset(Xy, feature_index, threshold)\n",
        "\n",
        "        # skip the node which has any on type 0. because this means it is already pure.\n",
        "        if len(true_xy) > 0 and len(false_xy) > 0:\n",
        "          # find the y values.\n",
        "          y = Xy[:, -1]\n",
        "          true_y = true_xy[:, -1]\n",
        "          false_y = false_xy[:, -1]\n",
        "          # calculate the impurity function.\n",
        "          impurity = self.impurity_function(y, true_y, false_y)\n",
        "\n",
        "          # if the calculated impurity is larger than save this value for comaparison (highest gain).\n",
        "          if impurity > largest_impurity:\n",
        "            largest_impurity = impurity\n",
        "            best_question = (feature_index, threshold)\n",
        "            best_datasplit = {\n",
        "              \"leftX\": true_xy[:, :n_features],   # X of left subtree\n",
        "              \"lefty\": true_xy[:, n_features:],   # y of left subtree\n",
        "              \"rightX\": false_xy[:, :n_features],  # X of right subtree\n",
        "              \"righty\": false_xy[:, n_features:]   # y of right subtree\n",
        "            }\n",
        "\n",
        "    return largest_impurity, best_question, best_datasplit\n",
        "\n",
        "  def _build_tree(self, X, y, current_depth=0):\n",
        "    n_samples , n_features = X.shape\n",
        "    # Add y as last column of X\n",
        "    Xy = np.column_stack((X, y))\n",
        "    # find the Information gain on each feature each values and return the question which splits the data very well\n",
        "    if (n_samples >= self.min_sample_split) and (current_depth < self.max_depth):\n",
        "      # find the best split/ which question split the data well.\n",
        "      impurity, quesion, best_datasplit = self._find_best_split(Xy)\n",
        "      if impurity > self.min_impurity:\n",
        "        # Build subtrees for the right and left branch.\n",
        "        true_branch = self._build_tree(best_datasplit[\"leftX\"], best_datasplit[\"lefty\"], current_depth + 1)\n",
        "        false_branch = self._build_tree(best_datasplit[\"rightX\"], best_datasplit[\"righty\"], current_depth + 1)\n",
        "        return DecisionNode(impurity=impurity, feature_index=quesion[0], threshold=quesion[1],\n",
        "                            left=true_branch, right=false_branch)\n",
        "\n",
        "    leaf_value = self._leaf_value_calculation(y)\n",
        "    return LeafNode(value=leaf_value)\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    self.root = self._build_tree(X, y, current_depth=0)\n",
        "\n",
        "  def predict_sample(self, x, tree=None):\n",
        "    if isinstance(tree , LeafNode):\n",
        "      return tree.prediction_value\n",
        "\n",
        "    if tree is None:\n",
        "      tree = self.root\n",
        "    feature_value = x[tree.feature_index]\n",
        "    branch = tree.right\n",
        "\n",
        "    if isinstance(feature_value, int) or isinstance(feature_value, float):\n",
        "      if feature_value >= tree.threshold:\n",
        "        branch = tree.left\n",
        "    elif feature_value == tree.threshold:\n",
        "      branch = tree.left\n",
        "\n",
        "    return self.predict_sample(x, branch)\n",
        "\n",
        "  def predict(self, test_X):\n",
        "    x = np.array(test_X)\n",
        "    y_pred = [self.predict_sample(sample) for sample in x]\n",
        "    y_pred = np.array(y_pred)\n",
        "    return y_pred\n",
        "\n",
        "  def draw_tree(self):\n",
        "    self._draw_tree(self.root)\n",
        "\n",
        "  def _draw_tree(self, tree = None, indentation = \" \", depth=0):\n",
        "    if isinstance(tree , LeafNode):\n",
        "      print(indentation,\"The predicted value -->\", tree.prediction_value)\n",
        "      return\n",
        "    else:\n",
        "      print(indentation,f\"({depth}) Is {tree.feature_index}>={tree.threshold}?\"\n",
        "            f\": {self.criterion_name}:{tree.impurity:.2f}\")\n",
        "      if tree.left is not None:\n",
        "          print (indentation + '----- True branch :)')\n",
        "          self._draw_tree(tree.left, indentation + \"  \", depth+1)\n",
        "      if tree.right is not None:\n",
        "          print (indentation + '----- False branch :)')\n",
        "          self._draw_tree(tree.right, indentation + \"  \", depth+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPk_h82Ihpzi"
      },
      "outputs": [],
      "source": [
        "tree = DecisionTreeClassifierFromScratch(max_depth=3)\n",
        "# tree = DecisionTreeClassifier(max_depth=3, criterion=\"gini\")\n",
        "tree.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "at-n9USNh0jY"
      },
      "outputs": [],
      "source": [
        "tree.predict([[2.5,1.7]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nc8LCVa6h3Mn"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "y_t_pred = tree.predict(X)\n",
        "accuracy_score(y, y_t_pred)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
