{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlSeix_wg30m"
      },
      "outputs": [],
      "source": [
        "# Homework 4_1\n",
        "# ข้อมูล\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from sklearn import datasets\n",
        "# iris = datasets.load_iris()\n",
        "# X = iris[\"data\"][:,:]\n",
        "# y = (iris[\"target\"] == 2).astype(int)\n",
        "\n",
        "#OPTIONAL CODE: Convert the iris dataset inton a Pandas DataFrame structure to view it nicely\n",
        "# dfiris = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
        "#                      columns= iris['feature_names'] + ['target'])\n",
        "# dfiris\n",
        "\n",
        "!mkdir -p ~/.kaggle  # Create the directory if not exists\n",
        "!mv kaggle.json ~/.kaggle/  # Move the API key to the correct location\n",
        "!chmod 600 ~/.kaggle/kaggle.json  # Secure the file\n",
        "\n",
        "# Download the dataset\n",
        "!kaggle datasets download -d datasetengineer/logistics-and-supply-chain-dataset --unzip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('dynamic_supply_chain_logistics_dataset.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "xAC4O-MlhB43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionNode:\n",
        "  def __init__(self, impurity=None, feature_index=None, threshold=None, left=None, right=None):\n",
        "    self.left = left\n",
        "    self.right = right\n",
        "    # The largest impurity value of this node\n",
        "    self.impurity = impurity\n",
        "    # Index of the feature which make the best fit for this node.\n",
        "    self.feature_index = feature_index\n",
        "    # The threshold value for that feature to make the split.\n",
        "    self.threshold = threshold\n",
        "\n",
        "class LeafNode:\n",
        "  def __init__(self, value):\n",
        "    self.prediction_value = value\n",
        "\n",
        "class DecisionTreeClassifierFromScratch:\n",
        "  def __init__(self, min_sample_split=3, min_impurity=1e-7, max_depth=10, criterion='gini'):\n",
        "    self.root = None\n",
        "    self.min_sample_split = min_sample_split\n",
        "    self.min_impurity = min_impurity\n",
        "    self.max_depth = max_depth\n",
        "    self.impurity_function = self._calculate_information_gain\n",
        "    if criterion == 'entropy':\n",
        "      self.criterion = self._entropy\n",
        "      self.criterion_name = criterion\n",
        "    else:\n",
        "      self.criterion = self._gini_index\n",
        "      self.criterion_name = 'gini'\n",
        "\n",
        "  def _gini_index(self, y):\n",
        "    gini = 1\n",
        "    unique_value = np.unique(y)\n",
        "    for val in unique_value:\n",
        "      # probability of that class.\n",
        "      p = np.sum(y == val) / len(y)\n",
        "      gini += -np.square(p)\n",
        "    return gini\n",
        "\n",
        "  def _entropy(self, y):\n",
        "    entropy = 0\n",
        "    unique_value = np.unique(y)\n",
        "    for val in unique_value:\n",
        "      # probability of that class.\n",
        "      p = np.sum(y == val) / len(y)\n",
        "      entropy += -p * np.log2(p)\n",
        "    return entropy\n",
        "\n",
        "  def _calculate_information_gain(self, y, y1, y2):\n",
        "    # :param y: target value.\n",
        "    # :param y1: target value for dataset in the true split/right branch.\n",
        "    # :param y2: target value for dataset in the false split/left branch.\n",
        "\n",
        "    # propobility of true values.\n",
        "    p = len(y1) / len(y)\n",
        "    info_gain = self.criterion(y) - p * self.criterion(y1) - (1 - p) * self.criterion(y2)\n",
        "    return info_gain\n",
        "\n",
        "  def _leaf_value_calculation(self, y):\n",
        "    most_frequent_label = None\n",
        "    max_count = 0\n",
        "    unique_labels = np.unique(y)\n",
        "    # iterate over all the unique values and find their frequentcy count.\n",
        "    for label in unique_labels:\n",
        "      count = len( y[y == label])\n",
        "      if count > max_count:\n",
        "        most_frequent_label = label\n",
        "        max_count = count\n",
        "    return most_frequent_label\n",
        "\n",
        "  def _partition_dataset(self, Xy, feature_index, threshold):\n",
        "    col = Xy[:, feature_index]\n",
        "    X_1 = Xy[col >= threshold]\n",
        "    X_2 = Xy[col < threshold]\n",
        "\n",
        "    return X_1, X_2\n",
        "\n",
        "  def _find_best_split(self, Xy):\n",
        "    best_question = tuple()\n",
        "    best_datasplit = {}\n",
        "    largest_impurity = 0\n",
        "    n_features = (Xy.shape[1] - 1)\n",
        "    # iterate over all the features.\n",
        "    for feature_index in range(n_features):\n",
        "      # find the unique values in that feature.\n",
        "      unique_value = set(s for s in Xy[:,feature_index])\n",
        "      # iterate over all the unique values to find the impurity.\n",
        "      for threshold in unique_value:\n",
        "        # split the dataset based on the feature value.\n",
        "        true_xy, false_xy = self._partition_dataset(Xy, feature_index, threshold)\n",
        "\n",
        "        # skip the node which has any on type 0. because this means it is already pure.\n",
        "        if len(true_xy) > 0 and len(false_xy) > 0:\n",
        "          # find the y values.\n",
        "          y = Xy[:, -1]\n",
        "          true_y = true_xy[:, -1]\n",
        "          false_y = false_xy[:, -1]\n",
        "          # calculate the impurity function.\n",
        "          impurity = self.impurity_function(y, true_y, false_y)\n",
        "\n",
        "          # if the calculated impurity is larger than save this value for comaparison (highest gain).\n",
        "          if impurity > largest_impurity:\n",
        "            largest_impurity = impurity\n",
        "            best_question = (feature_index, threshold)\n",
        "            best_datasplit = {\n",
        "              \"leftX\": true_xy[:, :n_features],   # X of left subtree\n",
        "              \"lefty\": true_xy[:, n_features:],   # y of left subtree\n",
        "              \"rightX\": false_xy[:, :n_features],  # X of right subtree\n",
        "              \"righty\": false_xy[:, n_features:]   # y of right subtree\n",
        "            }\n",
        "\n",
        "    return largest_impurity, best_question, best_datasplit\n",
        "\n",
        "  def _build_tree(self, X, y, current_depth=0):\n",
        "    n_samples , n_features = X.shape\n",
        "    # Add y as last column of X\n",
        "    Xy = np.column_stack((X, y))\n",
        "    # find the Information gain on each feature each values and return the question which splits the data very well\n",
        "    if (n_samples >= self.min_sample_split) and (current_depth < self.max_depth):\n",
        "      # find the best split/ which question split the data well.\n",
        "      impurity, quesion, best_datasplit = self._find_best_split(Xy)\n",
        "      if impurity > self.min_impurity:\n",
        "        # Build subtrees for the right and left branch.\n",
        "        true_branch = self._build_tree(best_datasplit[\"leftX\"], best_datasplit[\"lefty\"], current_depth + 1)\n",
        "        false_branch = self._build_tree(best_datasplit[\"rightX\"], best_datasplit[\"righty\"], current_depth + 1)\n",
        "        return DecisionNode(impurity=impurity, feature_index=quesion[0], threshold=quesion[1],\n",
        "                            left=true_branch, right=false_branch)\n",
        "\n",
        "    leaf_value = self._leaf_value_calculation(y)\n",
        "    return LeafNode(value=leaf_value)\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    self.root = self._build_tree(X, y, current_depth=0)\n",
        "\n",
        "  def predict_sample(self, x, tree=None):\n",
        "    if isinstance(tree , LeafNode):\n",
        "      return tree.prediction_value\n",
        "\n",
        "    if tree is None:\n",
        "      tree = self.root\n",
        "    feature_value = x[tree.feature_index]\n",
        "    branch = tree.right\n",
        "\n",
        "    if isinstance(feature_value, int) or isinstance(feature_value, float):\n",
        "      if feature_value >= tree.threshold:\n",
        "        branch = tree.left\n",
        "    elif feature_value == tree.threshold:\n",
        "      branch = tree.left\n",
        "\n",
        "    return self.predict_sample(x, branch)\n",
        "\n",
        "  def predict(self, test_X):\n",
        "    x = np.array(test_X)\n",
        "    y_pred = [self.predict_sample(sample) for sample in x]\n",
        "    y_pred = np.array(y_pred)\n",
        "    return y_pred\n",
        "\n",
        "  def draw_tree(self):\n",
        "    self._draw_tree(self.root)\n",
        "\n",
        "  def _draw_tree(self, tree = None, indentation = \" \", depth=0):\n",
        "    if isinstance(tree , LeafNode):\n",
        "      print(indentation,\"The predicted value -->\", tree.prediction_value)\n",
        "      return\n",
        "    else:\n",
        "      print(indentation,f\"({depth}) Is {tree.feature_index}>={tree.threshold}?\"\n",
        "            f\": {self.criterion_name}:{tree.impurity:.2f}\")\n",
        "      if tree.left is not None:\n",
        "          print (indentation + '----- True branch :)')\n",
        "          self._draw_tree(tree.left, indentation + \"  \", depth+1)\n",
        "      if tree.right is not None:\n",
        "          print (indentation + '----- False branch :)')\n",
        "          self._draw_tree(tree.right, indentation + \"  \", depth+1)"
      ],
      "metadata": {
        "id": "5pf3c8gihEtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tree = DecisionTreeClassifierFromScratch(max_depth=3)\n",
        "# tree = DecisionTreeClassifier(max_depth=3, criterion=\"gini\")\n",
        "tree.fit(X, y)"
      ],
      "metadata": {
        "id": "LPk_h82Ihpzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tree.predict([[2.5,1.7]])"
      ],
      "metadata": {
        "id": "at-n9USNh0jY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "y_t_pred = tree.predict(X)\n",
        "accuracy_score(y, y_t_pred)"
      ],
      "metadata": {
        "id": "Nc8LCVa6h3Mn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}